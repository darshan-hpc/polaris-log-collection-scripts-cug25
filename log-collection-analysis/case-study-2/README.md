**find-interesting-jobs-stdio.py**

This script analyzes aggregate STDIO job data to find interesting jobs to analyze. In particular, it prints out the 10 highest intensity jobs (in terms of total bytes).

`Usage: python find-interesting-jobs-stdio.py <job_stats_dir>`

`<job_stats_dir>` is the directory containing all the CSV files generated by the `extract-job-stats.py` script.

The job in Case Study 2, 1977553-17460120447186390966.darshan, was selected for being the highest intensity job identified by this script.

The log file is included here for convenience. A PyDarshan job summary report can be generated to reproduce I/O cost and I/O operation count figures, Figure 7 and Figure 8, respectively.

`python -m darshan summary 1977553-17460120447186390966.darshan`

The analysis in the paper related to extensive re-reading of application data can be reproduced manually, using a combination of the PyDarshan `file_stats` tool and `darshan-parser`.

The following PyDarshan command indicates the most read-intensive files accessed using STDIO: 

`python -m darshan file_stats --module=STDIO 1977553-17460120447186390966.darshan`

It identifies 8 per rank files that each account for over 66 TiB of read data. Choosing one of the files (e.g., /lus/grand/3073605787) and analyzing it with `darshan-parser` gives detailed counters captured by Darshan:

`darshan-parser ./1977553-17460120447186390966.darshan | grep /lus/grand/3073605787`

In particular, `STDIO_BYTES_WRITTEN`, `STDIO_BYTES_READ`, `STDIO_MAX_BYTE_WRITTEN`, and `STDIO_MAX_BYTE_READ` help support the analysis from the case study in the paper related to extensive re-reading of data.

**analyze-intensive-user.py**

This script summarizes aggregate STDIO statistics, as well as contributions by this particular intensive user. This data is used in Table 3.

`Usage: analyze-intensive-user.py <job_stats_dir>`

`<job_stats_dir>` is the directory containing all the CSV files generated by the `extract-job-stats.py` script.
